% Preamble
% ---
\documentclass[11pt]{report}

% Packages
% ---
\usepackage{utils}

\usepackage[utf8x]{inputenc} % Unicode support (Umlauts etc.)
\usepackage[english]{babel} % Change hyphenation rules
\usepackage{graphicx} % Add pictures to your document
\usepackage[pdfpagelabels]{hyperref} % Add a link to your document
\usepackage{soul}
\usepackage{cite}
\usepackage[a4paper, margin=2cm]{geometry}
\usepackage{listings}
\lstset{captionpos=b,numberbychapter=false}
\providecommand*{\listingname}{Listing}

\makeatletter
\AtBeginDocument{\let\c@listing\c@lstlisting}
\makeatother

\begin{document}

\begin{titlepage}
\begin{center}

    \begin{figure}[!htb]
        \begin{minipage}{0.14\textwidth}
            \includegraphics[width=\linewidth]{images/logo_ac.jpg}
        \end{minipage}\hfill
        \begin{minipage}{0.45\textwidth}
            Facultatea de Automatică și Calculatoare \newline
            Programul de Licență: \newline
            Calculatoare şi Tehnologia Informaţiei
        \end{minipage}\hfill
        \begin{minipage}{0.3\textwidth}
            \includegraphics[width=\linewidth]{images/logo_upt.jpg}
        \end{minipage}
    \end{figure}

    \vspace*{80px}

    {\Huge Haskzure}\\[4mm]
    {\Large \textit{Azure Resource Manager Bindings in Haskell}}\\

    \vspace{10mm}

    {\textsc Lucrare de licență} \\[3cm]

    \begin{flushright}
        \large Nashwan Azhari \\[1cm]
    \end{flushright}
    \begin{flushleft}
        \large
            \emph{Conducător științific} \\
            conf. dr. ing. Marius Minea \\[1cm]
    \end{flushleft}

    \vfill
    {\large {Timișoara \\June, 2016}}

\end{center}
\end{titlepage}

\date{\today{}}
\title{Haskzure: Bibliotecă de Haskell pentru Azure Resource Manager}
\author{Nashwan Azhari}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Abstract:
\begin{abstract}

We describe the process of designing and implementing Haskzure, a Haskell
library for managing Microsoft Azure through its set of Resource Manager APIs. The
main purpose of the library is to provide a bare minimum set of functionality
for performing CRUD operations on the more common compute and networking
resources available on Azure under a unified interface familiar to functional
programmers.
Considering the enourmous surface area of the ARM APIs (1100+ individual calls
available), emphasis was placed more on designing the library so as to make it
as easy to extend as possible versus focusing on feature-completeness. \newline
We will describe all the technical implications of writing a client library for
the ARM APIs, as well as  how Haskzure makes use of the common purely
functional programming idioms available in Haskell to achieve a high degree of
code reuse, and how the set of core components which provide the generic
interface for interacting with Azure are layed out and implemented.

\end{abstract}

\tableofcontents{}

\renewcommand\listoflistingscaption{Listings:}
\listoflistings

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Introduction:
\chapter{Introduction}

\section{Goal}

Considering the prevalence of Cloud Computing and its immense impact on the IT
industry as a whole, companies nowadays are commited to moving as much of their
IT workloads (infrastructure, storage arrays, big data processing etc..) `into
the cloud'. Whilst many choose to invest in their own datacenters and build
an in-house \textit{private cloud} solution, the vast majority prefer to turn
to so-called \textit{public clouds} for hosting all of their IT needs. As such,
the public cloud business is one of the most fast-growing industries in the
field, with huge names such as Amazon's AWS, Microsoft's Azure and Google's
Compute Engine offering a vast selection of both cloud infrastructure elements
and services to anyone in need. \newline
With this stunning growth in the industry and the shift to even more distributed
application architectures born to run in the cloud, the traditional model of a
company hiring a handful of system administrators to manage their
infrastructure and application lifecycle needs is no longer a feasible one.
As the cloud became a solution to programatically configuring IT
infrastructure, so, now, must there be a solution for configuring the clouds.
In this sense, the most prevalent approach is that of clouds providing a means of
remote management (most commonly HTTP-based REST APIs), through which, with the
aid of `language binding libraries' (aka SDKs), programmers may gain access to
and programatically deploy, modify and manage all the resources the cloud platform
offers. \newline
Considering that managing cloud infrastructure is an inherrently
transformational process, and that no programming paradigm better captures this
process than functional programming, we will focus on providing such a library
of language bindings for Microsoft's Azure public cloud platform (specifically,
the set of `Azure Resource Manager' APIs), in Haskell, one of the most
representative languages in the functional programming space.


\section{Motivation}

This thesis describes the design and implementation of a `Haskzure', a Haskell
library which provides various data structures and functions for interacting
with the `Azure Resource Manager' (or ARM) set of APIs for Microsoft's Azure
Cloud platform. \newline

The library should:
\begin{itemize}
    \item{} expose the standard set of CRUD operations for the most commonly
        used cloud resources available through the ARM APIs
    \item{} conform with the Haskell language's high-level and safe nature,
        making it easy to use within a functional context
    \item{} provide an easy way of extending the library's functionality to
        newer types of cloud resources
    \item{} include behavioral tests for all internal parts of code not
        directly interacting with Azure
\end{itemize}
Considering the large surface area of the ARM APIs (about 300 resource
types and over 1100 individual API calls), the emphasis is placed on a
good design of the core components for easy extensibility versus absolute
feature completeness. As will be shown, the abstractions available in Haskell
offer a straightforward means of making the core functionality completely
generic, and thus permitting for library features to be plugged in very easily.


\section{Related Work}

Cloud API binding libraries are commonplace for the more popular cloud
platforms, and, in most cases, these libraries are offered by the cloud vendor
themselves for a selection of the most used programming languages.
In the case of ARM, for example, there are open source libraries
\cite{listOfAzureSDKs} available from Microsoft for C\#, Go, Java, JavaScript,
PHP, Python and Ruby, provided with the intent of making it easier for
developers to manage the infrastructure under their Azure subscription. \newline
On the Haskell side, however, there are only a handful of such libraries. Most
notable would be a set of libraries all focused on interacting with Digital
Ocean, and one fairly limited but well-designed one for AWS\@
\cite{niceAwsSdk}. For Azure, the only Haskell-related tooling comes in the
form of a library which makes use of the now obsolete Azure Service Management
(or ASM) API \cite{distributedProcessAzure}. However, it only offers limited
management capabilities over ASM Cloud Services with the express intention
of being used with Cloud Haskell (a Haskell runtime designed to distribute
programs over multiple machines). \newline
As previously mentioned, the extremely large surface area of the ARM APIs make
implementing a fully-featured library a daunting task, which better motivates the
choice of Microsoft itself to almost fully auto-generate all of their libraries
through means detailed later. We have chosen, however, to use the levels of
abstraction provided by the Haskell language to move the boundary of code
generation farther away by hand-writing a core set of functionalities which are
designed to be as generic as possible and only generating the static data structures
those core operations operate upon.

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Theoretical Overview:
\chapter{Theoretical Overview}

\section{Haskell}

Haskell is a general-purpose, purely functional programming language developed
by a committee of researchers \cite{realworldhaskell} starting from the early
1990s. It is one of the most popular functional programming languages out there
as it lies as the forefront of research in the field. It is a language which
offers a handful of independent implementations, most of which compile Haskell to
native machine code. By far the most popular, however, is the
\textit{Glasgow Haskell Compiler} (or GHC), as it is the one being actively
developed by the committee, and thus the most up-to-date one with regards to the
latest developments in the language. It is also the compiler of choice for this
project, with more details provided in the section about tooling.
Haskell's combination of features makes it largely unique in terms of the
programming principles and techniques it offers, which we enumerate in the
subsections to follow:

\subsection{Purely Functional Programming Style}

The functional programming style has long been considered relatively cumbersome
due to its avoidance to mutable state and side-effects. As such, many have
argued that the cost of complete purity (in the mathematical sense, applied to
functions) far outweighs its benefits, and thus purely functional languages
will never actually `be useful'. Haskell manages to maintain purity throughout
by employing a number of `functional design patterns' (most ideas having been
borrowed from the field of \textit{Category Theory}), to deal with the likes of
mutable state and I/O while maintaining purity.\newline
The style lends itself very well to transformational problems (problems
involving numerous computations and transformations done on data), and as such
is very concise in the  modelling and management of cloud infrastructure.

\subsection{Strong Typing Discipline and Compiler Extensions}

Haskell is a statically typed language, with one of its main virtues being
its expressive type system. It is a system designed around
\textit{Algebraic Data Types} (or ADTs), and \textit{typeclasses}, which
define type-level constraints on the said ADTs. The system is very minimal
at heart, but it however allows for almost all of the diverse types,
constraints and their combinations which comprise the concise yet flexible
standard libraries. \newline
In addition, there are a handful of compiler extensions which either allow for
some added behavior to the typeclass system (MultiParamTypeClasses,
FlexibleContexts\ldots), or some extending of the data type system (ex:
DataKinds, TypeFamilies, GADTs etc\ldots). Although not part of the official standard,
most of the compiler extensions come packaged with GHC and enjoy widespread
use due to their added value to the language. When detailing the
implementation, we will also mention the more important extensions used in the
creation of Haskzure.

\subsection{Non-Strict Evaluation}

A programming language's `evaluation strategy' refers to the way expressions,
and operations which operate on expressions, are evaluated. More precisely, it is
largely concerned with \textit{when} the evaluation occurs. Most programming
languages employ a `strict evaluation' model, where a computation is performed
as soon as the expression is bound to a variable (but not necessarily
needed). This model works very well for most cases, but it does
carry the chance of there being more work done than needed (such as evaluating
expressions assigned to unused variables, for example). \newline
In languages with a `non-strict evaluation strategy', excess work is usually
prevented by various mechanisms. In Haskell, for example, the exact strategy
used is named \textit{call-by-need}, but is often referred to as `lazy
evaluation'. In effect, the value of an expression is only ever computed when
needed, a practice often cobined with \textit{memoization} to also cache
already performed computations so as to not recompute them (for example, the
base cases of tail-recursive functions can be very useful to cache). \newline
Despite having clear advantages (especially in terms of memory usage), lazy
evaluation often makes it harder to reason about the runtime behavior of
Haskell programs, especially in a concurrent \cite{concurrenthaskell} setting.
Haskell does of course offer a means of specifying the strictness of a value's
evaluation, a feature which may be employed when it is deemed safer to perform
computations up-front versus the default deferred manner (in fact,
\textit{Aeson} \cite{aeson}, the library we employ to encode and decode the
JSON payloads we intract with ARM through is inherrently strict).

\subsection{Category Theory Concepts}

Haskell borrows numerous concepts from \textit{Category Theory}, a branch of
mathematics dedicated to formalizing structures solely based on collections of
objects and arrows. Most notably, abstract constructs such as the
\textit{Functor}, \textit{Applicative Functor}, and the \textit{Monad}
are modelled as core typeclasses within Haskell to abstract numerous
non-functional concepts ranging from mutable state to exception handling
in a purely functional context (more precisely, as a binding together
of operations within a certain context which entails intermediate `gluing'
operations \cite{learnyouahaskell}). For Haskzure, we chose
to conform to the \textit{mtl} (standing for `Monad Transformers Library'),
which defines the standard set of Monad implementations used through most
Haskell code (similar to what the STL is for C++).
These powerful abstractions, despite giving the language a steeper
learning curve, allow for an amazing degree of conciseness if the problem
domain is properly modelled, as will be shown later.

\subsection{Template Haskell}

Metaprogramming is an integral part of any high-level programming language. In
Haskell's case, metaprogramming features come in the form of \textit{Template
Haskell}, a compiler extension and library which basically allow one to create
typed `splices' and insert them into the AST of a module before it gets
compiled. This allows for numerous use-cases, such as automatically declaring
data types and instantiating them to specific typeclasses, which is one of the
main drivers of Haskzure's implementation we will detail more in the next
chapter.


\section{Azure and the ARM APIs}

\subsection{Microsoft Azure}

Azure is the public cloud offering provided by Microsoft. It is an immensely
intricate system, with over 20 datacenters located worldwide serving a vast
array of compute, networking, storage and service resources to businesses and
even the US government. As such, it is a very hard system to manage
efficiently, as often the resources you are deploying are not only handled by
different Azure resource providers, but will likely get deployed on different
continets entirely. \newline
Like most public clouds, Azure offers a set of APIs for managing the resources
for your specific subscription. To be exact, Azure has more than one active API
facade at the user's disposal.

\subsection{The Azure Storage API}

The Azure Storage API offers access to manipulate `storage services', the
service-based approach to storage solutions offered by Azure. Storage services
are the container under which all other storage entities must live.
These include the likes of:
\begin{itemize}
    \item{} storage containers, which are equivalent to the directories of a
        filesystem by holding storage blobs
    \item{} storage blobs, which come in two flavors (namely block and page
        blobs), and are akin to files on a filesystem, but are made
        accessible via HTTP/HTTPS
    \item{} storage queues, which are the online messaging queue solution
        offered by Azure
    \item{} file storage services, which are separate services which provide
        file sharing capabilities via the SMB protocol
    \item{} table storage services, which offer storage to unstructured data
        in a similar manner to NoSQL databases
\end{itemize}
Alongside these, Azure also has a comprehensive offering of SQL Server
services, big data repositories (marketed as `Data Lake Store'), and
fully-fledged NoSQL database services. \newline
The Storage API is accessed via sending HTTPS requests with XML payloads,
(authentication being proven by virtue of special storage service keys), and
provides numerous configuration options (such as georeplication of storage
service contents, for example). For all the intents and purposes of Haskzure,
however, we will not be interacting with the Storage API at all, as all of our
few storage needs are also covered by the ARM APIs.

\subsection{The Azure Service Management API}

The Azure Service Management (or ASM) API was the initial cloud management API
for Azure since its launch in 2010. The API is RESTful, the means being XML
payloads sent over HTTPS with authentication done via thumbprints of X.509
certificates which had been preinstalled onto Azure and referenced in the
application.
Back in its early days, Azure was leaning more towards a Platform as a
Service-type system, with the main focus being on abstracting
away as many details of the infrastructure as possible and allowing developers
to deploy their application directly and let Azure manage its resource needs.
\newline
As such, the main unit of work under the old ASM API was the so-called `Cloud
Service', which represented a collection of virtual machines and their
connecting resources (network interfaces, virtual networks, etc\ldots) meant
specifically for said virtual machines. This model very much enforced an SOA
(Service-Oriented Architecture) on applications and posed a number of issues
within the API's design, most prevalent being the close ties between cloud
resources within a particular Cloud Service, but the relatively cumbersome
links with the rest of the infrastructure elements, which even lead to
questions of whether ASM was indeed fully REST-compliant or not
\cite{asmNetworkingIssue}. \newline
All in all, while the ASM API will likely still be available for use for a long
time to come, it is slowly but steadily being eclipsed by the ARM APIs.


\subsection{The Azure Resource Manager APIs}

The Azure Resource Manager (ARM) set of APIs, first released in 2015,
represents the new solution for programatically managing your infrastructure on
Azure. As opposed to ASM, ARM better fits the Infrastructure as a Service
role by focusing on providing operations for managing infrastructure elements
atomically as opposed to ASM's restrictive model of Cloud Services. It is better
thought of as a \underline{set} of multiple API facades unified under a single
top-level one. To be precise, the Azure team has made completely separate APIs
for the various types of cloud resource providers they offer (i.e.\ compute,
networking etc\ldots), each effectively having its own namespace under a grand
unified one dubbed the `Resource Manager', as opposed to ASM's flat namespace
implementation. The API is fully REST-compliant, with JSON-encoded payloads
being sent via HTTPS to the appropriate subpaths of the resource providers.
\textit{OAuth2} is used as the authorization framework, with your application
or Azure user having to be registered within an Azure \textit{Active
Directory} which acts the \textit{authorization server} for all the OAuth2
flows currently offered by Azure. \newline
ARM's only real enforcement when it comes to the resources you're deploying is
that they must all be allocated to a `Resource Group'. This is however nothing
more that a matter of labelling, as all resources may freely communicate with
others across resource groups (but not locations), the mechanism offering a
simple way of aggregating heterogeneous resources and performing operations on
them in bulk (like deleting all the resources in a group, for example\ldots).
\newline
As an added feature, the top-level `Resource Manager' provider offers the
ability to deploy a resource group by providing the JSON-encoded configuration
of all the resources desired in it, exactly as happens in the case Amazon's
Cloud Formation or OpenStack's Heat. Despite its handiness though, this
`template deployment' mode of operation cannot be really considered a complete
orchestration solution, as its main purpose is just deploying the resources,
after which no added management/monitoring is done. Of interest, however, is
the fact that the format of the resources encoded as JSON is almost identical
with the one through which we interact with the APIs, further proving that the
Resource Manager API is nothing more than a union of those of the individual
resource providers.

\section{Building an ARM Client Library}

\subsection{Technical Details}

In a nutshell, the ARM APIs work by sending the JSON-encoded payloads of
operations via HTTPS to the Resource Manager Rest API. The details
of this process are presented in the following sections.

\subsubsection{Authorization via Azure Active Directory}

The authorization framework of choice is \textit{OAuth2}, which is designed to
allow for easy implementation of authorization within applications without the
explicit need of users to provide username and password credentials to them. In
the case of ARM, Azure Active Directory resources play the role of the
\textit{authorization server} in all the supported OAuth2 flows. \newline
The following is a list of the OAuth2 authorization methods available against Azure AD:

\begin{itemize}
    \item{`Authorization Code' grant:} an authorization method in which the user
        of the application/service is redirected to Azure AD for
        authentication, after which Azure passes the application the
        authorization token via an application callback URL
    \item{`Client Credentials' grant:} this method requires that the application
        (i.e.\ the `client'), be previously registered within the authorization
        server, with access rights predefined and a given `client ID' and
        `client secret' (passkey) to be used for it to fetch its own
        authorization token
    \item{`Resource Owner Password Credentials' grant:} this method relies on
        the user handing the application his/her own credentials and the
        application (which needs to be previously registered with an associated
        `client ID' and permissions), and the application then authenticates
        with the owner's credentials to be granted a token
\end{itemize}

The scenario we will focus on is the `Client Credentials' grant, in which an
HTTP URL-encoded POST is performed on the authentication endpoint as shown
in \autoref{listing:tokenRequest}. \newpage

\begin{lstlisting}[
        caption={OAuth2 token request example.},
        label={listing:tokenRequest}
    ]
POST /<azure_ad_id>/oauth2/token?api-version=1.0 HTTP/1.1
Content-Type: application/x-www-form-urlencoded
Host: login.windows.net
Content-Length: 123

grant_type=client_credentials&resource=<resource>&
    client_id=<client_id>&client_secret=<client_secret>
\end{lstlisting}

Note that all of the parameters to be filled within the URL-encoded POST body
are part of the standard OAuth2 client credentials grant flow with the
`resource' we are requesting access to being, in all cases of authorization
against the Azure APIs, \url{https://management.core.windows.net/}.
If the provided information was correct and authorization is granted,
a JSON-encoded response containing the access token and some
additional information is returned to be sent as a header field in all
subsequent API calls requiring authorization. The structure of the
authorization response is presented in \autoref{listing:tokenResponse}.

\begin{listing}[H]
\caption{JSON structure of OAuth2 token response.}
\label{listing:tokenResponse}
\begin{minted}{json}
{
    "token_type": "Bearer",
    "access_token": "<token>",
    "expires_in": "<expiration_delta_seconds>",
    "expires_on": "<absolute_expiry_moment>",
    "not_before": "<minimum_absolute_expiry_moment>",
    "resource": "https://management.core.windows.net/"
}
\end{minted}
\end{listing}

\subsubsection{Operations through the HTTP-based REST APIs}

The ARM APIs are fully REST-compliant with all interactions done via HTTP\@. Each
particular resource type has its own URL at which HTTP requests may be
performed to issue certain actions. The general format of the resource URLs is
presented in \autoref{listing:resourceUrlFormat}.

\begin{lstlisting}[
        caption={URL format of an ARM resource on Azure.},
        label={listing:resourceUrlFormat}
    ]
https://management.azure.com/subscriptions/<subscription_id>
      /resourceGroups/<resource_group_name>/providers/<provider_name>
      /<resource_type>[/<resource_name>][/operation_name]
\end{lstlisting}

With each of the respective parameters being:

\begin{itemize}
    \item{subscription\_id}: the ID of the Azure subscription under which the
        resource should be created
    \item{resource\_group\_name}: the name of the resource group which contains
        the resource the operation is being performed on (recall that an
        ARM resource may not exist outside of a resource group)
    \item{provider\_name}: the name of the resource provider which handles that
        particular resource type. All resource provider names have the format
        `Microsoft.*', where * is the domain the provider handles (for
        example, there is a provider named `Microsoft.Compute' which handles
        compute resources such as virtual machines, `Microsoft.Network' for
        networking resources etc\ldots)
    \item{resource\_type}: the type of the resource we want to manipulate
        (`virtualMachines', for example)
    \item{resource\_name}: if we are performing an operation on a specific
        resource, we must also provide its name
    \item{operation\_name}: if performing a specific operation on a resource,
        we must provide the name of the operation (`redeploy' for a specific
        virtual machine, for example)
\end{itemize}

The vast majority of operations exposed through the ARM APIs follow the same
basic model of interaction with regards to the HTTP methods used, namely:

\begin{itemize}
    \item{GET}: issues an information \ul{retrieval} request for the resource
        specified in the URL:
        \begin{itemize}
            \item{} if the request URL describes a particular resource given by
                its name, the JSON-encoded properties of that resource are
                returned in the body of the response with HTTP 200, with a 404
                issued in case the resource does not exist
            \item{} if the request URL describes a whole type of resources, the
                response will contain a JSON-encoded list of all the individual
                resources of that type (in the given resource group)
        \end{itemize}
    \item{PUT}: issues a \ul{creation} request for a particular resource whose
        specific properties are provided via the JSON-encoded body, or, in case
        the resource already exists, its properties are \ul{updated} with the
        provided ones:
        \begin{itemize}
            \item{} if the operation takes effect immediately, HTTP 200 is
                returned, as well as a repeat of the details of the request
            \item{} if the operation takes time to finish, HTTP 201 is
                returned, as well as a repeat of the details of the request and
                the URL at which to poll until the operation has finished
        \end{itemize}
    \item{POST}: issues an \ul{operation} request for a particular resource
        (like powering a virtual machine on/off, for example):
        \begin{itemize}
            \item{} if the operation is accepted, HTTP 202 is usually returned,
                as well as a repeat of the details of the request
            \item{} most operations are modelled as long-running, with a polling
                URL provided within the response body
            \item{} status code 204 is also possible when there is no need for
                a response body to be returned
        \end{itemize}
    \item{DELETE}: issue a \ul{deletion} request for a particular resource:
        \begin{itemize}
            \item{} if the operation is accepted but it is a long-running
                operation (like deleting a virtual machine, for example), then
                HTTP 200 is usually returned alongside a polling URL in the
                response body
            \item{} if the operation should take effect immediately, then HTTP
                204 is usually returned with no response body
        \end{itemize}
\end{itemize}

\subsubsection{API Payload Format}

The payloads of the HTTP transactions with the ARM APIs are encoded in JSON
format and sent through the bodies of the HTTP requests/responses. The exact
contents often represents the properties of the resources that are being
queried, created or modified, which are specific to each resource in turn.
An example of the set of properties one can expect for an Azure
Availability Set, a resource which specifies replication properties for virtual
machines, can be found in \autoref{listing:availabilitySetJSON}.

\begin{listing}[H]
\caption{JSON representation of an Availability Set's properties.}
\label{listing:availabilitySetJSON}
\begin{minted}{json}
{
    "id": "<availability_set_id>",
    "name": "<availability_set_name>",
    "location": "availability_set_location",
    "type": "Microsoft.Compute/availabilitySets",
    "platformUpdateDomainCount": 5,
    "platformFaultDomainCount": 5,
    "virtualMachines": "[<list of virtual machines from the availability set>]"
}
\end{minted}
\end{listing}

Of special note is the fact that some properties are common for all ARM
resources, namely:

\begin{itemize}
    \item{id}: unique ID of the resource which is in fact the whole path
        following the Azure Management DNS name from the URL provided in
        \autoref{listing:resourceUrlFormat}.
    \item{name}: name of the resource, which may also be extracted from its ID
    \item{location}: the Azure location at which the resource has been
        deployed in normalized form (ex: `West US' is encoded as `westus')
    \item{type}: not mandatory for most transmissions as it is
        directly deductible from the request URL path. The type of the
        entity may also be provided in the form
        `ResourceProvider/resourceType'
\end{itemize}

Most noteworthy is that the exact details of how the payloads are constructed
per operation is not entirely standard. However, as described in the previous
section, the following guidelines apply to the structure of the payloads with
respect to the set of properties of the resource which constitutes the object
of our request:

\begin{itemize}
    \item{} sent as-is when a creation, update or operation request is
        performed for/on a single resource
    \item{} received as-is when a GET request is performed on a single
        resource
    \item{} received as members of a list found under the `values' key
        whenever a listing is requested (a GET performed on a resource type URL)
    \item{} nested within the properties of a different resource when
        operations are performed on it (ex: virtual subnets may either
        be defined on their own, or as sub-resources of the virtual networks
        they belong to)
\end{itemize}

\subsection{The Swagger API Description Format}

As seen in the previous section, there are a large amount of variables to take
into account when implementing a specific ARM API call, with a lot of aspects
not entirely consistent throughout the whole space of possible calls. As
such, most vendors which offer such broad APIs (1100+ calls for all the
providers on their latest API versions), often also publish all of the
available API calls in an API description format. In Azure's case, the ARM APIs
are documented \cite{azureApiSpecs} in a format known as \textit{Swagger}
(recently adopted by the `OpenAPI Initiative' \cite{openApiInitiative} and
now known as the  `OpenAPI Specification Format').
Swagger is an API description format based on JSON\@. Officially, the format only
exists as a JSON schema~\cite{swaggerSpecification} to which all Swagger files
must comply, while Swagger itself also has JSON schema-like properties (references
to other resources, files etc\ldots). Provided a complete Swagger description
of an API, we may find in it the following:

\begin{itemize}
    \item{} the \ul{base host} against which all other requests are made (in our
        case that will always be \url{https://management.core.windows.net/})
    \item{} the communication schema (\ul{protocol}) used (in our case,
        always HTTP)
    \item{} the \ul{communication format} through which interactions are
        performed (in our case, always either `application/json' or
        `text/json', encoded as UTF-8)
    \item{} the \ul{security/authorization} mechanism to be used and extra details
        about it (in our case, all the information presented in the section
        about authorization)
    \item{} the application \ul{paths} available on the server (detailed in the
        section about the ARM HTTP-based REST APIs)
    \item{} for each path, the available \ul{operations} to be performed on
        that path (GET, POST etc\ldots)
    \item{} for each operation, all of its associated \ul{parameters} (URL path
        parameters, query parameters, request body format)
    \item{} for each operation, all of its associated \ul{response details}
        (possible status codes, response format)
\end{itemize}

An extract of the description for the API call which creates/updates a new
Availability Set with respect to the latest version of the ARM APIs can be
found in \autoref{listing:availabilitySetCreateSwagger}.

\begin{listing}[H]
\caption{Swagger excerpt for Availability Set creation operation.}
\label{listing:availabilitySetCreateSwagger}
\begin{minted}{json}
"/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/
    providers/Microsoft.Compute/availabilitySets/{name}": {
    "put": {
        "operationId": "AvailabilitySets_CreateOrUpdate",
        "parameters": [{
                "name": "resourceGroupName",
                "in": "path",
                "required": true,
                "type": "string",
            }, {
                "name": "name",
                "in": "path",
                "required": true,
                "type": "string",
            }, {
                "name": "parameters",
                "in": "body",
                "required": true,
                "schema": {
                    "$ref": "#/definitions/AvailabilitySet"
                },
            }, {
                "$ref": "#/parameters/ApiVersionParameter"
            }, {
                "$ref": "#/parameters/SubscriptionIdParameter"
            }
        ],
        "responses": {
            "200": {
                "schema": {
                  "$ref": "#/definitions/AvailabilitySet"
                }
            }
        }
    }
}
\end{minted}
\end{listing}

This excerpt has been partially reformatted and some auxiliary details
have been left out (such as the `description' fields of each of the
parameters), but the rest is factually accurate. Also of note is that the
actual schema for the structure of the request body is merely a reference to
its respective definition within the schema (`\#/definitions/AvailabilitySet'),
which coincides with the structure presented in \autoref{listing:availabilitySetJSON}.


The Swagger description of the ARM APIs has proved invaluable in understanding
the features of the API and details of each operation. It is also where the
figures of 1100+ individual calls involving 300 or so individual resources were
extracted from.

\subsection{Swagger-based Client Library Generation}

Given the thorough Swagger description of the ARM APIs, it may seem almost
natural to try to derive as much client library functionality directly from the
specifications. As previously mentioned, this is the exact practice
Microsoft has employed for the collection of Azure client libraries they offer.

\subsection{Azure AutoRest}

The methodology by which the Azure team generates these libraries follows this
basic set of principles:

\begin{itemize}
    \item{} write a minimum set of foundation code by hand, which will be later
        reused in the generated code (this includes functionality such as the
        authentication, basic wrappers of the standard HTTP library from the
        language in question, and abstract base classes to inherit from later if
        in an Object-Oriented setting)
    \item{} use the Swagger description to generate the types/data structures
        which represent the transmission payloads (these are often referred to
        as the `models' and implement some generic interfaces for
        serialization/deserialization to/from JSON)
    \item{} use the resulting models to generate all of the code of the
        specific operations which use those models (ex: implementing the
        GET, PUT and DELETE operations based on the model of a virtual machine)
\end{itemize}

The last two points are handled by a Microsoft project known
as `Azure AutoRest'~\cite{azureAutoRest}. AutoRest is written in C\# and
generates code based on \textit{Razor templates}, a template markup syntax
which usually carries the file extension `cshtml'. The specific details of the
generated code are filled into the Razor template programatically and results
in the client library code.

Below is the basic architectural overview of an AutoRest-generated client
library in \autoref{listing:defaultClientLibraryArchitecture}.

\begin{listing}[H]
\caption{Architectural overview of AutoRest-generated client libraries.}
\label{listing:defaultClientLibraryArchitecture}
\begin{center}
    \begin{tabular}{ | p{7.5cm} | }
        \hline
        \textbf{auto-generated }code:
        \begin{itemize}
            \item{} glue logic
            \item{} operations
            \item{} type definitions
        \end{itemize} \\
        \hline
        simple wrappers (HTTP, JSON, OAuth2) \\
        \hline
        native libraries \\
        \hline
    \end{tabular}
\end{center}
\end{listing}

\subsubsection{Disadvantages of Code Generation}

Whilst code generation does bring the advantage of mediating all of the
particularities of some API calls, it does also bring some drawbacks. \newline
Firstly and obviously, code generation is inherently \ul{imperfect}. At the
start of the library generation initiative, the first results of AutoRest
were practically unusable as the generated code often did not even compile (in the
case of the Go, Java and C\# libraries), or were incorrect and led to runtime
crashes (in the case of the dynamic languages such as Python and Ruby). There
were many cases where the AutoRest system and even the Swagger specifications
themselves had to be retroactively refactored in order to better facilitate
support for each language in part. Even nowadays updates to the specification
still uncover some bugs in AutoRest which lead to the libraries being sub-par.
In \autoref{listing:pythonLibraryFailure}, you may see a simple example of a
situation where the code generation engine for the Python client library
completely disregards a key-value argument.

\begin{listing}[H]
\caption{Generated Python SDK issue.}
\label{listing:pythonLibraryFailure}
\begin{minted}{python}
def __init__(self, id=None, load_balancer_backend_address_pools=None,
        load_balancer_inbound_nat_rules=None, private_ip_address=None,
        private_ip_allocation_method=None, public_ip_address=None,
        provisioning_state=None, name=None, etag=None, **kwargs):
    super(NetworkInterfaceIPConfiguration, self).__init__(id=id, **kwargs)
    self.load_balancer_backend_address_pools = load_balancer_backend_address_pools
    self.load_balancer_inbound_nat_rules = load_balancer_inbound_nat_rules
    self.private_ip_address = private_ip_address
    self.private_ip_allocation_method = private_ip_allocation_method
    # NOTE: the kwarg is ignored and 'subnet' field
    # gets consistently set to None here:
    self.subnet = None
    self.public_ip_address = public_ip_address
    self.provisioning_state = provisioning_state
    self.name = name
    self.etag = etag
\end{minted}
\end{listing}

More details on the particular issue listed above can be found on the Github
issue tracker for the Python ARM SDK \cite{pythonSDKIssue}.

Secondly, the code generated by AutoRest can be extremely hard to read and
understand, as the intricacies of generated code are very hard to reason about
upon just examining the results. This is most apparent in the case of the
dynamic languages, where endless nested if's and returns of control litter the
code, especially due to the issue of long running versus non-long running
operations. This problem is so pronounced it is even recommended that people
turn to reading the Swagger specification directly to understand the operations
they are using instead of turning to the code itself. \newline
These issues prompted us to use the Swagger specification as
little as possible and instead use Haskell's abstraction mechanisms to write
generic code which can be reused across almost all API calls.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Implementation:
\chapter{Haskzure's implementation}

In this chapter we will provide all the relevant implementation details for
Haskzure as well as motivate the general design decisions which were taken.

\section{Haskzure's design}

As opposed to the way Microsoft has taken to auto-generating a large portion of
the codebase of the client libraries as shown in
\autoref{listing:defaultClientLibraryArchitecture} , one of the main goals of
Haskzure was avoiding code generation as much as feasibly possible. \newline
In a nutshell, Haskzure can be viewed as a layered system with the general
architecture presented in \autoref{listing:haskzureArchitecture}.

\begin{listing}[H]
\caption{Architectural overview of Haskzure.}
\label{listing:haskzureArchitecture}
\begin{center}
    \begin{tabular}{ | p{7.5cm} | }
        \hline
        high-level monadic API \\ \hline
        glue logic \\ \hline
        generic operations \\ \hline
        \textbf{auto-generated}: data type definitions \\ \hline
        native libraries \\ \hline
    \end{tabular}
\end{center}
\end{listing}

As can be seen, code generation has been limited solely to the generation of
the data types representing Azure resources, with all the layers above it being
parametrically polymorphic with respect to specific type of resource they are
manipulating thanks to the design of the `Resource' ADT outlined in the next
section. This allows for a much more robust system, with the entirety of the
code directly interacting with Azure being hand-written.

\section{Generating the Resource Data Structures}

In the following section we will describe in detail all the steps taken from
Swagger definition to Haskell data type. For this purpose, the
`Availability Set' Azure resource was chosen for its simple structure and
the ease of demonstration it provides.

\subsection{Base Resource Type}

The first logical step towards abstraction is, of course, the factoring out of
common behavior. In order to facilitate abstraction of the resource attributes
shared among all resource definitions, Swagger permits the usage of a special
\texttt{allOf} field which denotes the `inclusion' of all properties of
the referenced schema into the referencing one. For example, the whole
definition of an `Availability Set' is shown in
\autoref{listing:availabilitySetSwagger}.

\begin{listing}[H]
\caption{Swagger definition of an Availability Set.}
\label{listing:availabilitySetSwagger}
\begin{minted}{json}
"AvailabilitySet": {
    "properties": {
        "properties": {
            "x-ms-client-flatten": true,
            "$ref": "#/definitions/AvailabilitySetProperties"
        }
    },
    "allOf": [
        {
            "$ref": "#/definitions/Resource"
        }
    ],
}
\end{minted}
\end{listing}

As may be noticed from the above excerpt, the Azure team encapsulated the
common properties shared among all Azure resource types in a single, so-called
`Resource' definition. The remaining resource-specific properties are also
defined separately in a schema with the same name as the resource data type
but, with `Properties' appended to it, which we will discuss more thoroughly
in the next next section.

The exact structure of the common properties defined in the base
Resource may be seen in \autoref{listing:resourceSwagger}.

\begin{listing}[H]
\caption{Swagger definition of the base `Resource' type.}
\label{listing:resourceSwagger}
\begin{minted}{json}
"Resource": {
    "properties": {
        "id": {
            "readOnly": true,
            "type": "string"
        },
        "name": {
            "readOnly": true,
            "type": "string"
        },
        "type": {
            "readOnly": true,
            "type": "string"
        },
        "location": {
            "type": "string"
        },
        "tags": {
            "type": "object",
            "additionalProperties": {
                "type": "string"
            }
        }
    },
    "x-ms-azure-resource": true
}
\end{minted}
\end{listing}

\newpage

From the definition of the base Swagger Resource, we may conclude that the
following fields are shared among all resources:

\begin{itemize}
    \item{} the resource ID which uniquely identifies the resource as
        the full resource path following the base host of the URL
        we are making API calls to in order to manipulate it as seen back in
        \autoref{listing:resourceUrlFormat}
    \item{} the name of the resource
    \item{} the resource's type, which is formed by joining the type of the
        resource with the resource provider in charge of it (for example
        `Microsoft.Network/virtualNetworks' for virtual network resources)
    \item{} the normalized Azure location where the resource is located
    \item{} additional key-value pairs representing tags for easy custom
        category of resources
\end{itemize}

This way of defining resource types as a Swagger `mixin' is something done
consistently throughout all of the ARM Swagger specifications, with the
properties specific to the resource in question being defined as a separate
entity each time. Based on this simple specification, we may go ahead and
define the base `Resource' ADT of our library, shown in
\autoref{listing:resourceHaskell}.

\begin{listing}[H]
\caption{Definition of Haskzure's base \texttt{Resource} type.}
\label{listing:resourceHaskell}
\begin{minted}{haskell}
data Resource a = Resource {
    -- | resID is the ID of the Resource:
    resID         :: String,

    -- | resName is the Name of the Resource:
    resName       :: String,

    -- | resGroup is the Name of the Resource Group in which the 'Resource' lives.
    resGroup      :: String,

    -- | resLocation is the Location of the Resource:
    resLocation   :: String,

    -- | resType is the Type of Resource:
    resType       :: String,

    -- | resProperties are the properties of the Resource:
    resProperties :: a
} deriving (Show)
\end{minted}
\end{listing}

Of special note is the type of the `resProperties' field. The `Resource'
data type is parametrically polymorphic with regards to the properties it
contains, no matter the resource type for which those may be. This will allow
for the base functions to be generic with regards to the resource type on which they
operate, as will be outlined in a later section. Additionally, we have
defined a `resGroup' field for conveniently specifying the resource group in which
the resource in question has been deployed.

\subsection{Resource Properties Data Types}

As seen in the architectural overview presented in
\autoref{listing:haskzureArchitecture}, the only code generated based on the
Swagger specification is that of the definitions of the data structures
representing the properties of a resource on Azure. These are the data structures
which will later be directly serialized/deserialized to/from the payloads of
all the HTTP interactions with ARM. \newline

The basic outline for data type generation based on the Swagger definition of a
`model' can be summarized as follows:

\begin{itemize}
    \item{} the resulting data type will have the same name as the Swagger
        definition, but with the `Properties' tag removed
    \item{} the resulting data type will have a single value constructor, whose
        name coincides with that of the data type in accordance with Haskell
        idioms
    \item{} the resulting value constructor will be of record type, with all of
        the fields representing each attribute in the Swagger definition
    \item{} each field's label features as a prefix the name of the data type so
        as to avoid naming collisions throughout the generated code
\end{itemize}

Following the above rules, we may now perform a simple case study on a sample
resource type, namely the `Availability Set' resource we have presented in
\autoref{listing:availabilitySetSwagger}. The Swagger definition of the
`AvailabilitySetProperties' of can be found in
\autoref{listing:availabilitySetPropertiesSwagger}.

\begin{listing}[H]
\caption{Swagger definition for the properties of an Availability Set.}
\label{listing:availabilitySetPropertiesSwagger}
\begin{minted}{json}
"AvailabilitySetProperties": {
    "properties": {
        "platformUpdateDomainCount": {
            "type": "integer",
            "format": "int32",
            "description": "Gets or sets Update Domain count."
        },
        "platformFaultDomainCount": {
            "type": "integer",
            "format": "int32",
            "description": "Gets or sets Fault Domain count."
        },
        "virtualMachines": {
            "type": "array",
            "items": {
                "$ref": "#/definitions/SubResource"
            }
        }
    },
}
\end{minted}
\end{listing}

The above Availability Set properties will result in the generation of a
data structure with the formal definition shown in
\autoref{listing:availabilitySetHaskell}.

\begin{listing}[H]
\caption{Generated Haskell definition of an Availability Set.}
\label{listing:availabilitySetHaskell}
\begin{minted}{haskell}
data AvailabilitySet = AvailabilitySet {
    availabilitySetPlatformUpdateDomainCount :: Int,
    availabilitySetPlatformFaultDomainCount :: Int,
    availabilitySetVirtualMachines :: [SubResource]
 } deriving (Show, Eq)
\end{minted}
\end{listing}

As one might imagine, the level of technical risk undertaken in the generation
of simple data structures is relatively small, as opposed to the generation
of a lot of code with its associated logic. Considering the limited scope
of these data structure definitions compared to the scope of the whole
project, it is easy to realize the benefits of only extracting the data
definitions from the Swagger specification.

\subsection{Serialization Instances}

Once the generation of a data structure representing the properties of a
resource has been performed, we must then provide instance implementations for
all of the typeclasses required in order to succesfully use said data structure
in interactions with Azure. Most notable are the typeclasses required to be
implemented by  \textit{Aeson} (a Haskell library for working with JSON-encoded
data), namely ToJSON and FromJSON, as well as an instance for the Monoid
typeclass which is commonly used in Haskell.

\subsubsection{ToJSON}

ToJSON is the Aeson typeclass implemented by all types which are serializable
to JSON. Its definition is very simple at heart and can be seen in
\autoref{listing:ToJSONHaskell}.

\begin{listing}[H]
\caption{Definition of Aeson't ToJSON typeclass.}
\label{listing:ToJSONHaskell}
\begin{minted}{haskell}
class ToJSON a where
    toJSON     :: a -> Value
    toEncoding :: a -> Encoding
\end{minted}
\end{listing}

We may see the existence of two main functions defined within the ToJSON
typeclass. \texttt{toJSON}, the first, is a function which takes a value
of the type in question and returns a value of type \texttt{Value}, a type
defined in the Aeson library which has its own ToJSON instance.
\texttt{toJSON} may thus be viewed as a function which encodes to JSON in two
separate steps by first `transcoding' the value of the given type into one of
type \texttt{Value}, which is then later actually serialized to JSON\@.
The other, \texttt{toEncoding}, is the more efficient variant, it taking a
value of the type in question and encoding it directly to JSON\@. By default, a
generic \texttt{toEncoding} which makes use of \texttt{toJSON} is used if only
the latter is provided.  Of course, the preferred option would be making use
of \texttt{toEncoding} for all of our serialization needs in order to reap
the added performance benefits. \newline
Fortunately, Aeson includes helper functions for making both a \texttt{toJSON}
and a \texttt{toEncoding} function provided the type we are instantiating is an
instance of a Haskell typeclass called \texttt{Generic}, which, even more
fortunately for us, is also automatically derivable with the language
extension `DeriveGeneric' available in GHC\@. The signature of
\texttt{genericToEncoding}, the function which, when partially applied to a set
of encoding \texttt{Options} results in the definition of \texttt{toEncoding}
we desire, is shown in \autoref{listing:genericToEncodingHaskell}.

\begin{listing}[H]
\caption{Signature of Aeson's \texttt{genericToEncoding}.}
\label{listing:genericToEncodingHaskell}
\begin{minted}{haskell}
genericToEncoding :: (Generic a, GToEncoding (Rep a)) => Options -> a -> Encoding
\end{minted}
\end{listing}

The next problem we were faced with was the proper choosing of the encoding
\texttt{Options} for our purposes. Recall that when we generated the properties
data structures, we intentionally prepended the name of the data type in order to
avoid naming collisions. As such, we must take care to choose the correct
encoding options which will lead to the field prefix being removed
when serializing to JSON\@. Furthemore, the options shall be clearly dependand on
the name of the data type, and thus a custom set of them must be created for
every data type we want to instantiate to ToJSON\@. The code performing the
process of generating the resource-specific \texttt{Options} can be seen in
\autoref{listing:mkEncodingOptionsHaskell}.

\begin{listing}[H]
    \caption{The Aeson encoding \texttt{Options} used.}
\label{listing:mkEncodingOptionsHaskell}
\begin{minted}{haskell}
baseAesonOptions :: Options
baseAesonOptions =  Options {
    fieldLabelModifier = id,
    constructorTagModifier = id,
    allNullaryToStringTag = True,
    omitNothingFields = True,
    sumEncoding = defaultTaggedObject,
    unwrapUnaryRecords = False
 }

mkFieldLabelPrefixRemoveModifier :: Name -> String -> String
mkFieldLabelPrefixRemoveModifier n = lowerFirst . drop (length $ showName n)
    where lowerFirst (c:cs) = toLower c : cs

mkEncodingOptions :: Name -> Options
mkEncodingOptions n = baseAesonOptions {
    fieldLabelModifier = mkFieldLabelPrefixRemoveModifier n
 }
\end{minted}
\end{listing}

The exact selections for the options are motivated as follows:

\begin{itemize}
    \item{} fieldLabelModifier: a function which describes the transformation
        to be applied to the labels of the data structure's fields. This is
        where we create a custom function for each data structure using
        \texttt{mkFieldLabelPrefixRemoveModifier}
    \item{} sumEncoding: parameters on how to encode sum data types. Not
        applicable to our case, as all of our generated data structures have a
        single value constructor
    \item{} constructorTagModifier: a function to be applied to the tags made
        from the names of the value constructors. Not applicable so the identity
        function \texttt{id} is used
    \item{} allNullaryToStringTag: specifies whether to encode values of types
        with all nullary value constructors (such as \texttt{Bool}) with just
        the names of the constructors with \texttt{constructorTagModifier}
        applied to them. Set to \texttt{True} for simple enum-like types
    \item{} omitNothingFields: specifies wether to omit \texttt{Nothing} values
        in case our fields are of type \texttt{Maybe a}. Not applicable, but
        set to \texttt{True} for futureproofing
    \item{} unwrapUnaryRecords: specifies whether or not to unrap the value of
        a type with a single value constructor of record type, containing a
        single record field (such as newtypes). Not applicable, set to the
        default of \texttt{False}
\end{itemize}

With that, the full definition of the Template Haskell function
\texttt{toJSONInst}, as well as a basic example of how it is used with the
`AvailabilitySet' ADT, may be found in \autoref{listing:toJSONInstHaskell}.

\begin{listing}[H]
\caption{The \texttt{toJSONInst} Template Haskell function.}
\label{listing:toJSONInstHaskell}
\begin{minted}{haskell}
toJSONInst :: Name -> Q [Dec]
toJSONInst name =
    [d| instance ToJSON $( conT name ) where
            toEncoding = genericToEncoding $ mkEncodingOptions name
      |]

-- in a separate file:
{-# LANGUAGE TemplateHaskell #-}

toJSONInst ''AvailabilitySet
\end{minted}
\end{listing}

\subsubsection{FromJSON}

FromJSON is the Aeson typeclass representing data types deserializable from
JSON\@. It contains a single member, namely \texttt{parseJSON}, as seen in
\autoref{listing:FromJSONHaskell}.

\begin{listing}[H]
\caption{Aeson's FromJSON typeclass.}
\label{listing:FromJSONHaskell}
\begin{minted}{haskell}
class FromJSON a where
    parseJSON :: Value -> Parser a
\end{minted}
\end{listing}

As we can see, \texttt{parseJSON} is polymorphic in its return type, as well as
returning the value wrapped inside the \texttt{Parser} monad Aeson defines. As
such, we must make use of Aeson't library functions in order to write our
\texttt{parseJSON} definition, which effecively `decodes' an already parsed
Aeson \texttt{Value} into the data structure of type `a'. Fortunately, there is
also a \texttt{genericParseJSON} available for use, which has the signature
presented in \autoref{listing:genericParseJSONHaskell}.

\begin{listing}[H]
    \caption{Signature of Aeson's \texttt{genericParseJSON}}
\label{listing:genericParseJSONHaskell}
\begin{minted}{haskell}
genericParseJSON :: (Generic a, GFromJSON (Rep a)) => Options -> Value -> Parser a
\end{minted}
\end{listing}

With the above, and provided the same set of \texttt{Options} we used to create
the \texttt{toEncoding} function (Aeson performs a `lookup' when decoding, so
the same \texttt{Options} used the encode the fields are the ones needed to see
how to expect the fields to be encoded), we may easily define
\texttt{parseJSON}. \newline
More interesting, however, is what happens when some fields of the
data structure are not present in the payload being decoded. In this case,
Aeson's default behavior is to throw an error which leads to a program crash.
To prevent stability issues and provide sensible defaults for empty fields when
decoding, we must implement our own fallback mechanism. This is illustrated in
\autoref{listing:withDefaultsHaskell}.

\begin{listing}[H]
\caption{\texttt{withDefaults} Parser merging function.}
\label{listing:withDefaultsHaskell}
\begin{minted}{haskell}
-- | Merges two Aeson 'Value's via 'Map' 'Monoid' instance.
(<+>) :: Value -> Value -> Value
Object x <+> Object y = Object (x <> y)
_ <+> _ = error "<+>: merging non-objects"

-- | Returns new parsing function which will use the specified 'Pair's as
-- default values for the data type being decoded's fields.
withDefaults :: (Value -> Parser a) -> [Pair] -> Value -> Parser a
withDefaults parser defs js@(Object _) = parser (js <+> object defs)
withDefaults _ _ _ = empty
\end{minted}
\end{listing}

As can be seen, \texttt{withDefaults} employs the Monoid instance of Map (which
is used in the underlying definition of Aeson \texttt{Value} type) to unify the
object getting parsed with an object containing a list of pairs of field names
and their respective default value. The union of the parsed and default objects
occurs before the parsing into the destination data structure.
As mentioned, the default object is constructed from the list of Aeson
\texttt{Pair}s within which we must refer to fields with the same
name we'd expect them to be found in the JSON payload (in
this case, without the data type name prefix we put on all record fields). This
is done through the Template Haskell function \texttt{mkFromJSONPairs} whose
implementation we won't detail. Either way, \texttt{mkFromJSONPairs} is the
main driver for the \texttt{fromJSONInst} Template Haskell function whose
expected outcome for the `AvailabilitySet' resource type shown in
\autoref{listing:availabilitySetHaskell} should be consistent with the one
presented in \autoref{listing:availabilitySetFromJSONInstanceHaskell}.

\begin{listing}[H]
\caption{AvailabilitySet FromJSON instance.}
\label{listing:availabilitySetFromJSONInstanceHaskell}
\begin{minted}{haskell}
instance FromJSON AvailabilitySet where
    parseJSON = (genericParseJSON (mkEncodingOptions "AvailabilitySet")
                `withDefaults` [
                    ("platformUpdateDomainCount" .= (mempty :: Int)),
                    ("platformFaultDomainCount" .= (mempty :: Int)),
                    ("virtualMachines" .= (mempty :: [SubResource]))
                ]) :: (Value -> Parser AvailabilitySet)
\end{minted}
\end{listing}

All of the default values supplied are actually the result of calling
\texttt{mempty} from the Monoid class for the respective type of the
field. The exact details of how \texttt{fromJSONInst} is implemented are beyond
the scope of this writing, but instead may be found better documented in the
Haddock documentation of Haskzure \cite{haskzureHaddocks}.

\subsubsection{Monoid}

Monoid instances for the resource data types serve two main purposes:

\begin{itemize}
    \item{} they are required in the case of nested Azure resources in order
        for their \texttt{mempty} to be used in the FromJSON instantiations as
        has been outlined in
        \autoref{listing:availabilitySetFromJSONInstanceHaskell}
    \item{} they are generally useful for both: \begin{itemize}
        \item{} making default empty values of the resources data types which
            may then be updated with what the desired characteristics are
        \item{} taking two resource configurations and \texttt{mappend}ing them
            together
    \end{itemize}
\end{itemize}

These instances are made using the facilities of the \texttt{generic-deriving}
\cite{packageGenericDeriving} package with the same constraint of the data type
needing to be an instance of \texttt{Generic} still applying. The full
definition of the \texttt{monoidInst} Template Haskell function is outlined
in \autoref{listing:monoidInstHaskell}.

\begin{listing}[H]
\caption{The \texttt{monoidInst} Template Haskell function.}
\label{listing:monoidInstHaskell}
\begin{minted}{haskell}
monoidInst :: Name -> Q [Dec]
monoidInst name =
    [d| instance Monoid $( conT name ) where
            mempty = memptydefault
            mappend = mappenddefault
      |]
\end{minted}
\end{listing}

As can be observed, we rely on \texttt{memptydefault} and
\texttt{mappenddefault} in our Monoid instance, which provide sensible default
implementations for data structures which are instances of \texttt{Generic}.
\newline
Finally, we may quickly and easily create a function which dispatches to the
Template Haskell functions for creating instances for both ToJSON and FromJSON,
as well as the Monoid one. The resulting function is the one used on all of the
data types which were generated in the previous layer, and may be seen in
\autoref{listing:mkAllInstsHaskell}.

\begin{listing}
\caption{Definition of the \texttt{mkAllInsts} Template Haskell function.}
\label{listing:mkAllInstsHaskell}
\begin{minted}{haskell}
mkAllInsts :: Name -> Q [Dec]
mkAllInsts name = fmap concat $ mapM ($ name) [monoidInst, toJSONInst, fromJSONInst]
\end{minted}
\end{listing}

\section{OAuth2 Authorization} % 2 pages
    % describe AzAD app permissions setup
    % oauth2 details
    % code samples for token refreshing etc...

With the resource data structures defined, we may now focus on the
\textit{OAuth2} authorization. Regardless of the authorization flow used,
the response should be the same and consistent with the structure
shown in Chapter 2 in \autoref{listing:tokenResponse}. As mentioned, in Haskzure
specifically, the only currently supported OAuth2 flow is the `Client
Credentials flow'.

\subsection{The Credentials ADT}

Presuming Haskzure has been properly setup within an instance of Azure
Active Directory (a straightforward process done via the Azure online user
interface), the set of information required in order to succesfully
authenticate using the `Client Credentials' OAuth2 flow against Azure AD has
been aggregated into a data structure named \texttt{Credentials}, whose
definition can be found in \autoref{listing:credentialsHaskell}.

\begin{listing}[H]
\caption{Haszure's \texttt{Credentials} ADT.}
\label{listing:credentialsHaskell}
\begin{minted}{haskell}
data Credentials = Credentials {
    tenantId       :: BS.ByteString,
    clientId       :: BS.ByteString,
    subscriptionId :: BS.ByteString,
    clientSecret   :: BS.ByteString
 } deriving Show
\end{minted}
\end{listing}

The record fields of the \texttt{Credentials} ADT all play a part the
authorization process, and namely:

\begin{itemize}
    \item{`tenantId'}: the unique ID of the Azure Active Directory against
        which to authenticate, as will be presented later
    \item{`clientId'}: unique ID number of the application (in this case
        Haskzure). This is obtained when the application is registered within
        the Azure AD
    \item{`clientSecret'}: secret string to be used to authenticate the client.
        This is automatically generated and also obtained when the application
        is registered within the Azure AD
    \item{`subscriptionId'}: the ID of the subscription under which the
        operations will be performed
\end{itemize}

\subsection{The Token ADT}

Considering the well-defined properties of an OAuth2 authorization token, we
may go ahead and define the \texttt{Token} ADT to be used in all functions
requiring authorization. Also considering that the response for succesfull
token aquisition is the same (namely, the JSON structure presented in
\autoref{listing:tokenResponse}), we also went ahead and declared a FromJSON
instance for \texttt{Token} so as to be able to directly deserialize the token
response into it. Both the definition of \texttt{Token} and its FromJSON
instance may be found in \autoref{listing:tokenHaskell}.

\begin{listing}[H]
\caption{Haskzure's \texttt{Token} ADT and its FromJSON instance.}
\label{listing:tokenHaskell}
\begin{minted}{haskell}
data Token = Token {
    token     :: BS.ByteString,
    tokenType :: BS.ByteString,
    expiresOn :: Integer
 } deriving Show

instance FromJSON Token where
    parseJSON (Object o) = Token <$>
                           o .: "access_token" <*>
                           ((o .: "expires_on") >>=
                               (return . (read :: String -> Integer))) <*>
                           o .: "token_type"
    parseJSON _ = fail "Token must be deserialized from a JSON object."
\end{minted}
\end{listing}

\subsection{Token Fetching and Refreshing}

As described in the Theoretical Overview chapter, the process of fetching a
token for the `Client Credentials' OAuth2 grant is nothing more than performing
a URL-encoded POST with the appropriate query parameters as shown in
\autoref{listing:tokenRequest}. Thus the most crucial part is the correct
forging of the query parameters by extracting what is required from the
\texttt{Credentials} data structure. The exact process of creating the query
parameters for the token request is shown in
\autoref{listing:mkTokenParamsHaskell}.

\begin{listing}[H]
\caption{\texttt{mkTokenRequestParams} query parameter builder function.}
\label{listing:mkTokenParamsHaskell}
\begin{minted}{haskell}
mkTokenRequestParams :: Credentials -> QueryParams
mkTokenRequestParams creds = [
    ("resource", "https://management.core.windows.net/"),
    ("grant_type", "client_credentials"),
    ("client_id", clientId creds),
    ("client_secret", clientSecret creds)
 ]
\end{minted}
\end{listing}

Token requests must be directed to the OAuth2 path of the Azure AD's login URL
(\url{https://login.microsoftonline.com/<tenant_id>/oauth2/token}, where
`tenant\_id' is the ID of the Azure AD instance we are authenticating against,
thus the need for its supplying from the \texttt{Credentials} data structure).
The rest of the process of actually performing the URL-encoded POST and
deserializing the response is as straightforward as is to be expected.

\subsection{Authorizing with the Token}

After the \texttt{Token} has been succesfully recieved, using it for
authorization involves nothing more than adding an HTTP header to all
subsequent API calls. All encryption is performed by TLS (no ARM API calls
may come through plain HTTP), so the real purpose of the OAuth2 token is to
provide a verifiable sign of owning the necessary permission to perform the
requested action. \newline
In Haskzure, we made use of the \texttt{http-client}\cite{packageHTTPClient}
packages in which we have a \texttt{Request} data type which is
manipulated in order to set various attributes of the upandcoming request.
The operations required in order to set the appropriate request parameters
may be found in \autoref{listing:authorizationHeaderHaskell}.

\begin{listing}[H]
\caption{Necessary additions to the \texttt{Request} data type for authroization.}
\label{listing:authorizationHeaderHaskell}
\begin{minted}{haskell}
mkResourcePath :: String -> Resource a -> String
mkResourcePath subId res = printf ((mkResourceTypePath subId res) ++ "/%s") (resName res)

mkTokenHeader :: Token -> Header
mkTokenHeader t = (hAuthorization, (tokenType t) `BS.append` " " `BS.append` (token t))
\end{minted}
\end{listing}

Specifically, the above code adds a header with the key `Authorization', and
the value `token\_type token', where the format parameters correspond to
the type of the OAuth2 token as well as the token itself which are extracted
from the \texttt{Token} data structure.

\section{The Resource Manipulation Functions}

With the data structures and token retrieval parts taken care of, the next
logical layer of the library consists of the base functions which offer the
basic set of functionality needed for interfacting with Azure. In a nutshell,
the base set of functions include the Azure resource CRUD functions, as well as
a handful others for performing specific operations on some resource types (like
restarting a virtual machine, for example). In the following we will detail the
building of the base operations layer.

\subsection{Building the Resource Request URL}

The first step of performing an ARM request is ensuring that the request is
being made to the appropriate URL, as a lot of information is transmitted to
the ARM APIs through the application path being accessed. Some details of
the format of the ARM resource paths may also be found in the Technical Overview
section in \autoref{listing:resourceUrlFormat}, which is duplicated below in
\autoref{listing:resourceUrlFormat2}.

\begin{lstlisting}[
        caption={URL format of an ARM resource on Azure},
        label={listing:resourceUrlFormat2}
    ]
https://management.azure.com/subscriptions/<subscription_id>
      /resourceGroups/<resource_group_name>/providers/<provider_name>
      /<resource_type>[/<resource_name>][/operation_name]
\end{lstlisting}

Of special interest this time is determining where all the parameters should be
provided. The only required piece of information from the \texttt{Credentials}
data structure is the `subscription\_id' path element, whilst all
the rest of the elements may be directly extracted from the \texttt{Resource a}
we are operating on (which also explains the need for the \texttt{resGroup}
field of the Resource data type). \newline
The exact implementation of the URL building process is trivial, involving
nothing more that some smart string formatting.

\subsection{Function Definitions}

All of the base functions (\texttt{createOrUpdate}, \texttt{get},
\texttt{delete}) follow the same general behavior outlined below:

\begin{itemize}
    \item{} fetch a fresh API token from Azure AD
    \item{} build the request URL based on the parameters of the Resource being
        operated upon
    \item{} create the appropriate request body by serializing the provided
        Resource (as is the case for \texttt{createOrUpdate}, for example)
    \item{} perform the request to the appropriate URL over HTTPS
    \item{} await the server's response, which may in some cases last minutes
        depending on the scale of the request being performed
    \item{} if needing to return something, properly deserialize the response
        body into the required type
\end{itemize}

With that general methodology clear, the exact implementation details of the
CRUD functions are unnecessary. Their signatures, however, may be found in
\autoref{listing:baseFunctionsSignaturesHaskell}.

\begin{listing}[H]
\caption{Signatures of the base functions for manipulating resources.}
\label{listing:baseFunctionsSignaturesHaskell}
\begin{minted}{haskell}
get            :: (FromJSON a) => Credentials -> Resource a -> IO (Resource a)
createOrUpdate :: (ToJSON a) => Credentials -> Resource a -> IO ()
delete         :: Credentials -> Resource a -> IO ()
\end{minted}
\end{listing}

\chapter{Testing}

There are two main areas where testing should be focused: the data type and instance
generating Template Haskell functions, as well as some integration tests
which actually interact with Azure. \newline
The data type and instance generation code is, albeit kept to a minimum, an
important testing area as issues in the definition of the resource data types
may lead to a host of problems, especially in the case of the Aeson typeclass
instance generation functions. As such, Haskzure has a small suite of QuickCheck
tests for validating that the instance generation code is performing as
expected. \newline
On the other hand, the are no fully-blown integration tests available, but a
proposed solution of setting up an integration testing framework will be
presented.

\section{Validation of Instance Generation Code through QuickCheck}

In this section, we will describe how validation of the instance generation
code is performed using QuickCheck behavioral tests on mock data structures.
We'd like to be able to test the generated code as thoroughly as
possible to ensure encoding/decoding is done properly. In this sense, we may
identify the following major aspects we'd expect of the generated definitions:

\begin{itemize}
    \item{} encoding and the decoding should yield a result identical to the
        input data structure (encode/decode symmetry)
    \item{} encode/decode symmetry should also apply if the type of a field of
        a data structure is another Azure resource data structure
\end{itemize}

Whilst these may seem like properties which should be intrinsic to the Aeson
library itself, the fact that the Template Haskell functions generating our
ToJSON and FromJSON instances are parametrized over the name of the data type
means that there may easily be some discrepancy between the encoding and
decoding process which we'd like to actively test for. As an added benefit,
the testing of the Aeson instance generators also entitles testing the Monoid
instance generator as will be seen later.

\subsection{QuickCheck}

QuickCheck is a Haskell library designed to facilitate property-based testing
on randomly generated input. Most of QuickCheck's power is drawn from the
ability to instantiate types to QuickCheck's \texttt{Arbitrary} typeclass in
order to be able to test functions operating on values of that data type.
The full definition of the \texttt{Arbitrary} typeclass may be found in
\autoref{listing:arbitraryClassHaskell}.

\begin{listing}[H]
\caption{Definition of QuickCheck's \texttt{Arbitrary} typeclass.}
\label{listing:arbitraryClassHaskell}
\begin{minted}{haskell}
class Arbitrary a where
    arbitrary :: Gen a
\end{minted}
\end{listing}

As may be seen, the return type of \texttt{arbitrary} is a value wrapped in
QuickCheck's Gen monad within which arbitrary values are generated. It is
usually straightforward to provide an \texttt{Arbitrary} instance for a data
type, as you would just have to lift the value constructor into the Gen monad
and apply it to \texttt{arbitrary} values of the types the data type is
composed of.

For the Availability Set data type listed in
\autoref{listing:availabilitySetHaskell}, for instance, the \texttt{Arbitrary}
instance would look something along the lines of what is shown in
\autoref{listing:availabilitySetArbitraryHaskell}.

\begin{listing}[H]
\caption{\texttt{Arbitrary} instance for the AvailabilitySet data type.}
\label{listing:availabilitySetArbitraryHaskell}
\begin{minted}{haskell}
instance Arbitrary AvailabilitySet where
    arbitrary = AvailabilitySet <$>
                    (arbitrary :: Int) <*>
                    (arbitrary :: Int) <*>
                    (arbitrary :: [SubResource])
\end{minted}
\end{listing}

Fortunately, we may reuse some of the data type introspection code from the
FromJSON instance generation Template Haskell function to write a new function
which generates \texttt{Arbitrary} instances. More details on this helper
function may be found by consulting the project's Haddock
\cite{haskzureHaddocks} documentation.

\subsection{Validating the Aeson Instance Generation}

Provided an \texttt{Arbitrary} instance for our datatype, we can now write a
simple QuickCheck property to ensure the symmetry of encoding and decoding. The
property applicable to the Availability Set resource shown in
\autoref{listing:availabilitySetHaskell} , which merely compares the result
of applying Aeson's \texttt{endcode} and \texttt{decode} functions with the
original AvailabilitySet value, can be seen in
\autoref{listing:aesonSymmetryPropertyHaskell}.

\begin{listing}[H]
\caption{QuickCheck property regarding symmetry of JSON encoding/decoding.}
\label{listing:aesonSymmetryPropertyHaskell}
\begin{minted}{haskell}
{-# LANGUAGE TemplateHaskell #-}

arbiInst ''AvailabilitySet

prop_AvailabilitySetAesonSymmetry :: AvailabilitySet -> Bool
prop_AvailabilitySetAesonSymmetry :: d =
    ((fromJust . (decode :: BSL.ByteString -> Maybe AvailabilitySet) . encode) d) == d
\end{minted}
\end{listing}

To be precise, the data type we are testing against does not matter, as the
desired property will be validated/invalidated just the same on a data type
actually representing an Azure resource or on a dummy data type. What does
matter, however, is the proper covering of the case of nested data types (in
which the field of one type is of another complex data type). As such, among
there are two tests in which dummy types, some nesting others, have
the ToJSON and FromJSON instance generation functions used on them and a
property similar to the one shown above defined for each of the data types.
QuickCheck then randomly generates a set amount of values of said data types
and verifies the property for each input and reports back the result.

\subsubsection{Baseline Test}

Naturally, the Aeson encode/decode symmetry property is entirely dependant on
the implementation within the library to be valid. In order to verify that
issues or not on Aeson's side, there is a side-test in which the same property
is applied for Aeson's \texttt{Value} type for which an \texttt{Arbitrary}
instance was written by hand. In the event there be some issue with Aeson's
encode/decode operations, this regression-like check will properly alert to it.

\section{Proposed Integration Testing Setup}

Property-based testing is great when a need of certainty that a specific
code behavior is consistently shown, however it does not guarantee that the
property is compliant with what Azure expects. As such, the only real way to
be sure that both the generated data types and their instances are correct is
to actually attempt to interact with the ARM APIs. \newline
The result would be the equivalent of an integration testing suite which may be
ran periodically during Haskzure's lifecycle. The ideal target layer from the
architectural overview presented in \autoref{listing:haskzureArchitecture}
would the layer with the generic operations, as it is both the first layer to
be directly interacting with Azure, as well as the layer where the granularity
of operations is large (basically, at that layer each function has a direct
correspondence to an HTTP method). \newline
A good plan for the proposed integration testing suite would be as follows:

\begin{itemize}
    \item{} decide the set of properties for the tested operations (one for
        creation, another for deletion etc\ldots). These should be paramterized
        over the exact data type they are operating, as are the definition of
        the base operation within the library
    \item{} for each of the properties, write a Template Haskell function which
        generates properties of that type provided the name of the
        data structure representing the type of resource being tested
    \item{} a list of data type names for which properties should be
        generated and tested should be provided in order to start the testing
        process knowing what is being tested
\end{itemize}

The creation of an integration testing suite is invaluable to ensuring
Haskzure actually communicates cleanly with Azure, regardless of the exact
implementation details.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Using the library:
\chapter{Using the Library}

The Haskzure project may be found hosted on Github\cite{haskzure}.

\section{Installation and Importing}

Haskzure may be easily installed using a combination of git and the cabal
configuration management system. Having Haskzure installed is as simple as
running the shell statement outlined in \autoref{listing:installingHaskzure}.

\begin{lstlisting}[
        caption={Shell commands for installing and configuring Haskzure.},
        label={listing:installingHaskzure}
    ]
git clone https://github.com/aznashwan/haskzure &&
    cd haskzure && cabal configure &&
    cabal install
\end{lstlisting}

After installation, all of the following modules should be exposed and ready
for importing:

\begin{itemize}
    \item{} \texttt{Cloud.Haskzure.Core} defines all of the core components of
        Haskzure which present most of the library's functionality. This
        includes:
        \begin{itemize}
            \item{} the \texttt{Credentials} and \texttt{Token} authorization
                datatypes
            \item{} the \texttt{getToken} function for fetching an
                authorization token
            \item{} the Resource and SubResource ADTs with all of their
                constructors
            \item{} the base set of operations, including the generic Azure
                resource CRUD functions
        \end{itemize}
    \item{} \texttt{Cloud.Haskzure.Gen} defines all of the Template Haskell
        code in charge with resource data type and instance generation
        including:
        \begin{itemize}
            \item{} the \texttt{toJSONInst} and \texttt{fromJSONInst} Aeson
                typeclass instance generators
            \item{} the \texttt{monoidInst} Monoid instance generator
            \item{} general Template Haskell utilities for data type
                introspection with the purpose of creating instance definitions
                (most notably \texttt{recordFieldsInfo})
        \end{itemize}
    \item{} \texttt{Cloud.Haskzure.Resources} contains the definitions of all
        of the data types which represent resources on Azure (such as the
        \texttt{VirtualNetwork} or \texttt{VirtualMachine} data types)
\end{itemize}

\section{Typical Workflow}

Depending on the exact details of what is desired to be accomplished, a
standard workflow may be suggested. \newline

For example purposes, let's imagine we are faced with the following situation:

\begin{itemize}
    \item{} two virtual networks need creating following the same set of
        basic configuration options but in two different Azure locations
    \item{} after creation; both need to be modified to add a particular server's
        IP address to their list of DNS providers to be used by VMs in those
        networks
    \item{} we would like to return the resource IDs of the resulting virtual
        networks
\end{itemize}

The full code sample illustrating the example can be found in
\autoref{listing:haskzureExampleHaskell}.

As may be seen, the functional programming style lends itself well to problems
like manipulating sets of resources (here we just map the modification and
\texttt{createOrUpdate} function over the VNs within the IO monad). Overall,
the process of working with Haskzure can be summarized in the following steps:

\begin{itemize}
    \item{} if wanting to create a new resource or manipulate an existing one,
        a value of the properties type of that resource must be declared and
        wrapped inside the Resource ADT
    \item{} using the datatype, the base functions may be used in order to
        perform the desired operations on Azure
    \item{} all Azure-related actions happen within the IO monad, with error
        checking being done automatically behind the scenes
\end{itemize}

This methodology is consistent with usual Haskell idioms and comes across as
much more concise than the more procedural methodologies used in the client
libraries of the other languages.

\newpage

\begin{listing}
\caption{Code showcasing a common usage example for Haskzure.}
\label{listing:haskzureExampleHaskell}
\begin{minted}{haskell}
import Control.Monad
import Data.Default

import Cloud.Haskzure.Core      (createOrUpdate, Resource (..), mkResource,
                                 ResourceId)
import Cloud.Haskzure.Resources (VirtualNetwork(..), DhcpOptions(..), Subnet(..))

-- base configuration of all our virtual networks:
baseVN :: VirtualNetwork
baseVN = (mempty :: VirtualNetwork) {
    virtualNetworkDhcpOptions = DhcpOptions {
        dhcpOptionsDnsServers = ["8.8.4.4", "8.8.8.8"]
    },
    virtualNetworkAddressSpace = "10.7.0.0/16",
    virtualNetworkSubnets = [Subnet "10.7.0.0/24"]
}

-- we define a list of two virtual networks based on the above configuration;
-- one in the Western US Azure region; the other in Northern Europe:
virtualNetworks :: [Resource VirtualNetwork]
virtualNetworks = [
    mkResource $ baseVN { location = WestUS },
    baseVN { location = NorthEurope }
 ]

deployVirtualNetworks :: Credentials -> IO [ResourceId]
deployVirtualNetworks creds = do
    -- create our two virtual networks
    vns <- mapM (createOrUpdate creds) virtualNetworks

    -- now add "9.9.9.9" as a DNS server to both the virtual networks.
    -- for this we define a function:
    let newDnses = DhcpOptions (("9.9.9.9" :) .
                                dhcpOptionsDnsServers .
                                virtualNetworkDhcpOptions)

    -- and map it over our two virtual networks:
    vns <- mapM (\vn ->
            createOrUpdate creds $ vn {
                resProperties = resProperties vn {
                    virtualNetworkDhcpOptions = newDnses vn
                }
            ) vns

    return $ map resId vns

main :: IO ()
main = do
    -- note the initial passing in of the credentials:
    vnids <- deployVirtualNetworks credentials
    print $ "The virtual network ID's are: " ++ show vnids
\end{minted}
\end{listing}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Conclusions:
\chapter{Conclusion}

In order to better summarize what has been achieved, simple table
presenting some metrics related to line of code (or LoC) values
for both Haskzure's and the Python SDK's codebases may be found in
\autoref{listing:haskzurePythonComparison}.

\begin{listing}[H]
\caption{Comparison between Haskzure and the Python SDK's codebases.}
\label{listing:haskzurePythonComparison}
\begin{center}
\begin{tabular}{ | l | l | l | l | }
    \hline
    Project    & Feature Coverage (\% of 1100) & Total LoC & LoC for VM support \\
    \hline \hline
    Python SDK & 100\%                   & 114000  & 121
    (model\cite{pythonSDKVMModel}) + 1047 (operations\cite{pythonSDKVMOps}) \\ \hline
    Haskzure   & 3\%                     & 1200    & 7 (model) + 109
    (generic operations) \\
    \hline
\end{tabular}
\end{center}
\end{listing}

Comparing against the Python client library was chosen due to the conciseness
and dynamic typing of Python, which should result in the most compact codebase
possible in a procedural/object-oriented context. Naturally, the change of
paradigm from Python to Haskell plays an inevitable role in the dramatisation
of the statistics, however it is much more apparent that the design approaches
of the two project carries the most blame for the results we are seeing.
\newline
The following observations with regards to the presented statistics are worth
mentioning:

\begin{itemize}
    \item{} the total LoC metric is deceptive due to the descrepancy in
        features. However, in the case of Haskzure, the only step involved in
        `adding' management features of a new type of resource is defining its
        resource property data type (either by generating it or providing it
        upfront)
    \item{} comparing the code used for the actual `models' showcases
        differences in language capabilities. The two libraries obviously
        define the same model, however on the Python side a class must be
        declared alongside the constructor and a handful of other magic methods
    \item{} the 100 or so lines of code defining Haskzure's resource
        operation functions are generic and thus re-used for all types of
        resources, so in some ways for a fairer comparison to occur,
        they should be counted for each supported resource type as AutoRest
        generates the operations for each type separately (which is the source
        of the large difference in codebase sizes of the two projects)
\end{itemize}

The resulting system unequivocally shows that simple abstraction features
available in the language can go a long way into keeping the codebase small
and maintainable. While the scope is not as broad as the large auto-generated
libraries Microsoft offers for other languages, the conciseness achieved
by limiting the amount of generated code just to data type declarations leads
to much cleaner client library implementations in the long term.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% References:
\addcontentsline{toc}{chapter}{Bibliography and References}
\nocite{*}
\bibliographystyle{unsrt}
\bibliography{Diploma}

\end{document}
